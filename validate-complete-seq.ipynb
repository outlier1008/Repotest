{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c116f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docplex.mp.model import Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from docplex.util.environment import get_environment\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from preprocessing import PreprocessData\n",
    "\n",
    "pd.set_option('max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "853361e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeData(object):\n",
    "    def __init__(self, model, data, col):\n",
    "        self.model = model\n",
    "        self.data = data\n",
    "        self.col = col\n",
    "    def make_params(self, inds):\n",
    "        try:\n",
    "            new_data = self.data[self.col].groupby(by=inds).aggregate(lambda x: self.model.sum(x)).reset_index().rename(\n",
    "                columns={self.col: self.col.replace(self.col[: self.col.index('_')], 'sum')})\n",
    "        except ValueError:\n",
    "            new_data = self.data[self.col].groupby(by=inds).aggregate(lambda x: self.model.sum(x)).reset_index()\n",
    "        return new_data\n",
    "\n",
    "    def make_df(self, param, ind='time_period'):\n",
    "        if ind == 'time_period':\n",
    "            new_df = pd.DataFrame(columns=[ind]+self.col)\n",
    "        else:\n",
    "            new_df = pd.DataFrame(columns=self.col+[ind])\n",
    "        ele = [[i] * len(self.data) for i in param]\n",
    "        add_col = [i for e in ele for i in e]\n",
    "        for t in param:\n",
    "            new_df = pd.concat([new_df, self.data])\n",
    "        new_df[ind] = add_col\n",
    "        new_df.reset_index(inplace=True, drop=True)\n",
    "        return new_df\n",
    "\n",
    "    def split_cols(self, trans_col):\n",
    "        return self.data[self.col.strip('ID')+'_'+trans_col.strip('ID')].str.split(',', expand=True).rename(\n",
    "            columns={0:self.col, 1:trans_col}).apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    def get_dist(self, dist_name):\n",
    "        df = self.data.loc[(self.data['ID1_type']==self.col[0]) & (self.data['ID2_type']==self.col[1])][\n",
    "            ['ID1', 'ID2', 'distances']]\n",
    "        df.rename(columns={'ID1': self.col[0], 'ID2': self.col[1], 'distances': dist_name}, inplace=True)\n",
    "        for i in self.col:\n",
    "            if not df[i].isnull().any():\n",
    "                df[i] = df[i].astype(int)\n",
    "            else:\n",
    "                continue\n",
    "        return df\n",
    "\n",
    "def get_params(orig_df, param_df, params, ind=list(), repetitive=False, dropped=False):\n",
    "    #param_dict = {}\n",
    "    param_ = param_df[ind+params].set_index(ind)\n",
    "    orig_cols = orig_df.columns.tolist()\n",
    "    param_cols = param_df.columns.tolist()\n",
    "    incompatible_name = list(set(orig_cols).intersection(set(param_cols)))\n",
    "    if len(incompatible_name) > 0:\n",
    "        for n in incompatible_name:\n",
    "            param_.rename(columns={n: n+'_'}, inplace=True)\n",
    "    # 29/08/2022 -- added for repetitive keys when calculating precedences\n",
    "    if repetitive:\n",
    "        param_dict = dict()\n",
    "        param_dict[params[0]] = param_.groupby(ind)[params[0]].apply(list).to_dict()\n",
    "    else:\n",
    "        param_dict = param_.to_dict()\n",
    "    orig_df.set_index(ind, inplace=True)\n",
    "    for p in param_dict.keys():\n",
    "        orig_df[p] = orig_df.index.to_numpy()\n",
    "        orig_df[p] = orig_df[p].map(param_dict[p])\n",
    "    orig_df.reset_index(inplace=True)\n",
    "    if repetitive:\n",
    "        orig_df = orig_df.explode(params[0])\n",
    "    if dropped:\n",
    "        orig_df.dropna(inplace=True)\n",
    "    else:\n",
    "        orig_df.fillna(0, inplace=True)\n",
    "    return orig_df\n",
    "\n",
    "# dump location priority for dumping\n",
    "def outputDiagonal(lst, direction='secondary'):\n",
    "    # To print Primary Diagonal\n",
    "    if direction == 'primary':\n",
    "        return [lst[i][i] for i in range(len(lst))]\n",
    "    else:\n",
    "        return [lst[i][len(lst)-1-i] for i in range(len(lst))]\n",
    "\n",
    "def groupDiagonal(dim: dict()):\n",
    "    namesList = ['lift', 'priority', 'locationID']\n",
    "    total_arr = {}\n",
    "    priority_arr = {}\n",
    "    df_priority = pd.DataFrame()\n",
    "    for dump in dim.keys():\n",
    "        for i in range(len(dim[dump])):\n",
    "            add = np.sum([j**2 for j in dim[dump][:i]])\n",
    "            lst = [k + add for k in list(range(1, dim[dump][i]**2+1))]\n",
    "            arr = np.array(lst).reshape(-1, dim[dump][i])\n",
    "            priority_arr[i+1] = {}\n",
    "            for l1 in range(1, dim[dump][i]+1):\n",
    "                priority_arr[i+1][l1] = outputDiagonal(arr[:l1,:l1])\n",
    "            for l2 in range(-dim[dump][i]+1, 0):\n",
    "                priority_arr[i+1][-(-dim[dump][i]-l2)+dim[dump][i]] = outputDiagonal(arr[l2:, l2:])\n",
    "            total_arr[i+1] = arr\n",
    "\n",
    "        priority = pd.DataFrame.from_dict(priority_arr, orient='index').stack().explode().reset_index()\n",
    "        priority.columns = namesList\n",
    "        priority.insert(loc=0, column='dumpID', value=[dump]*len(priority))\n",
    "\n",
    "        df_priority = pd.concat([df_priority, priority])\n",
    "        df_priority['locationID'] = df_priority['locationID'].astype(int)\n",
    "    return df_priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e926ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(model, data_url, bin_num, grade_margin):\n",
    "\n",
    "    #bin_precedence = pd.ExcelFile(data_url + \"generatedFile.xlsx\")\n",
    "    \n",
    "    model.grade_margin = grade_margin\n",
    "    #model.df_blocks = pd.read_csv(data_url + \"1.block_model_as_mined.csv\")                        # block model\n",
    "    model.df_blocks = pd.read_csv(data_url + \"df_blocks_test2.csv\")                        # block model\n",
    "    model.df_blocks = model.df_blocks.loc[(model.df_blocks.z < 352.5)]\n",
    "    #model.df_blocks = model.df_blocks.loc[model.df_blocks.mineID==3]\n",
    "    model.df_blocks['vol'] = model.df_blocks['quantity'] / model.df_blocks['density']\n",
    "    \n",
    "    df_dumps = pd.read_csv(data_url + \"2.waste_dump_new3.csv\")                                # waste dumps\n",
    "    df_dumps['dump'+'_'+'location'] = df_dumps[['dumpID', 'locationID']].apply(lambda x: ','.join(x.values.astype(str)),\n",
    "                                                                               axis=1)\n",
    "    \n",
    "    stockpile_cap = pd.read_csv(data_url + \"5.stockpile_capacity.csv\")                   # stockpile bin capacity\n",
    "    \n",
    "    #model.dumping_priority = pd.read_csv(data_url + \"dumping_priority.csv\")\n",
    "    cell_cnt_key = 'cell_cnt'\n",
    "    cell_matrix = pd.DataFrame(df_dumps.groupby(['dumpID', 'z']).locationID.nunique().apply(np.sqrt)).rename(\n",
    "        columns={'locationID': 'cell_cnt'})\n",
    "    cell_matrix = cell_matrix.reset_index(level=['z'], drop=True)\n",
    "    cell_matrix[cell_cnt_key] = cell_matrix[cell_cnt_key].astype('int')\n",
    "    dump_priority = groupDiagonal(cell_matrix.groupby(level=0).agg(list).to_dict()[cell_cnt_key])\n",
    "    \n",
    "    # dumping_priority\n",
    "    priority = dump_priority.rename(columns={'locationID':'precedenceID'})\n",
    "    post_dumping_priority = dump_priority.rename(columns={'priority':'post_priority'})\n",
    "    post_dumping_priority['priority'] = post_dumping_priority['post_priority'] - 1\n",
    "    dumping_priority = get_params(post_dumping_priority, priority, params=['precedenceID'],\n",
    "                                  ind=['dumpID', 'priority', 'lift'], repetitive=True)\n",
    "    dumping_priority = get_params(dumping_priority, df_dumps, params=['volume'], ind=['dumpID', 'locationID'])\n",
    "    model.dumping_priority = dumping_priority.loc[dumping_priority['precedenceID'] != 0]\n",
    "    blockPreprocess = PreprocessData(model.df_blocks)\n",
    "    dumpPreprocess = PreprocessData(df_dumps)\n",
    "    # create precedent relationships\n",
    "    df_block_precedence = blockPreprocess.precedence('mineID', 'blockID')\n",
    "    df_waste_precedence = dumpPreprocess.precedence('dumpID', 'locationID')\n",
    "    # 04/10 -- revised: only some of the dumps can accommodate active materials\n",
    "    active_dumps = dumpPreprocess.detect_active('dumpID', 'locationID', [2])               # active info of dumps\n",
    "    model.df_dumps = get_params(df_dumps, active_dumps, params=['fill_active'], ind=['dumpID', 'locationID'])\n",
    "    # create suitable bins\n",
    "    #df_bins = suitable_bin(model.df_blocks, stockpile_cap, bin_num, grade_margin)\n",
    "    df_bins = blockPreprocess.suitable_bin(stockpile_cap, bin_num, grade_margin)\n",
    "    model.df_bins = get_params(df_bins.drop(columns=['possible_bin']), model.df_blocks,\n",
    "                               params=['if_active', 'quantity'], ind=['mineID', 'blockID'])\n",
    "    model.df_bins = model.df_bins.loc[((model.df_bins.binID<0) & (model.df_bins.if_active==0)) | (model.df_bins.binID>0)]\n",
    "    #model.df_bins = pd.read_csv(data_url + \"df_bins.csv\")\n",
    "\n",
    "    # get the capacity of each bin\n",
    "    model.bin_capacity_grade = model.df_bins[['binID', 'bin_capacity', 'avg_grade']].drop_duplicates()\n",
    "\n",
    "    # get the quantity of each block for block precedent conditions\n",
    "    model.mb_pre_cols = ['mineID', 'precedenceID', 'blockID']\n",
    "    model.df_block_precedence = get_params(df_block_precedence, model.df_blocks.rename(columns={'blockID': 'precedenceID'}),\n",
    "                                           params=['quantity'], ind=model.mb_pre_cols[:-1])\n",
    "    # get the capacity of each dump location for dump location precedent conditions\n",
    "    model.dl_pre_cols = ['dumpID', 'precedenceID', 'locationID']\n",
    "    model.df_waste_precedence = get_params(df_waste_precedence, df_dumps.rename(columns={'locationID': 'precedenceID'}),\n",
    "                                           params=['volume'],\n",
    "                                           ind=model.dl_pre_cols[:-1])\n",
    "\n",
    "    df_parameter = pd.read_csv(data_url + \"8.other_parameters.csv\")                     # parameters\n",
    "    model.other_params = dict(zip(df_parameter.parameters, df_parameter.value))\n",
    "    model.mining_cap = pd.read_csv(data_url + \"3.mining_capacity_new.csv\")                  # mining capacities\n",
    "    model.processing_cap = pd.read_csv(data_url + \"4.processing_capacity_new.csv\")          # processing capacities\n",
    "    df_distance = pd.read_csv(data_url + \"6.facility_distances.csv\")                    # facility distances\n",
    "    # distances\n",
    "    # i. distance_m_b_p\n",
    "    model.distance_mbp_ind = ['mineID', 'processorID']\n",
    "    model.distance_m_b_p = MakeData(model, df_distance, model.distance_mbp_ind).get_dist('distance_m_b_p')\n",
    "    # ii. distance_m_b_s\n",
    "    model.distance_mbs_ind = ['mineID', 'binID']\n",
    "    model.distance_m_b_s = MakeData(model, df_distance, model.distance_mbs_ind).get_dist('distance_m_b_s')\n",
    "    # iii. distance_m_b_d_l\n",
    "    model.distance_mbdl_ind = ['mineID', 'dumpID']\n",
    "    model.distance_m_b_d_l = MakeData(model, df_distance, model.distance_mbdl_ind).get_dist('distance_m_b_d_l')\n",
    "    # iv. distance_s_p\n",
    "    model.distance_sp_ind = ['binID', 'processorID']\n",
    "    model.distance_s_p = MakeData(model, df_distance, model.distance_sp_ind).get_dist('distance_s_p')\n",
    "    # iv. distance_s_d_l\n",
    "    model.distance_sdl_ind = ['binID', 'dumpID']\n",
    "    model.distance_s_d_l = MakeData(model, df_distance, model.distance_sdl_ind).get_dist('distance_s_d_l')\n",
    "\n",
    "    # make processing cost and recovery rate into a single column\n",
    "    pr_cols = ['mineID', 'blockID', 'grade', 'mining_cost', 'processing_cost_p1', 'processing_cost_p2',\n",
    "               'recovery_rate_p1', 'recovery_rate_p2']\n",
    "    processor_recovery = pd.wide_to_long(model.df_blocks[pr_cols],\n",
    "                                         i=pr_cols[:4],\n",
    "                                         stubnames=['processing_cost_p', 'recovery_rate_p'],\n",
    "                                         j='processorID').set_axis(['processing_cost', 'recovery_rate'],\n",
    "                                                                   axis='columns').reset_index()\n",
    "    model.processor_recovery = get_params(processor_recovery, model.df_bins, params=['binID', 'avg_grade', 'grade'],\n",
    "                                          ind=pr_cols[:2])\n",
    "    model.pro_rec_rate = (model.processor_recovery.processing_cost/model.processor_recovery.recovery_rate).unique()\n",
    "\n",
    "    model.rou = pd.read_csv(data_url + \"7.swell_factor.csv\")\n",
    "    # data\n",
    "    model.mine_block = model.df_blocks[['mineID', 'blockID', 'if_active', 'grade', 'density', 'wting',\n",
    "                                        'mining_cost', 'quantity']]\n",
    "    model.dump_location = model.df_dumps[['dumpID', 'locationID', 'dump_location', 'fill_active']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81e74507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_data(model, grade_zero=False):\n",
    "    # parameters\n",
    "    n_processors = int(model.other_params['n_processors'])\n",
    "    periods = int(model.other_params['time period'])\n",
    "    metal_price = model.other_params['unit metal price']                       # $/g\n",
    "    #metal_price = 2000  \n",
    "    refining_cost = model.other_params['unit refining cost']                   # $/g\n",
    "    delta_haulage_cost = model.other_params['delta haulage cost']              # $/tonne-bench (used when sent to waste dumps)\n",
    "    haulage_cost = model.other_params['unit haulage cost']                     # $/tonne-km\n",
    "    train_haulage_cost = model.other_params['train haulage cost (processing)'] # $/tonne-km\n",
    "    rehandling_cost = model.other_params['unit stockpile rehandling cost']     # $/tonne\n",
    "    rou = model.rou\n",
    "    # define the lower bound of grade that allows sending materials to processors as \"hard\" cut-off grade\n",
    "    model.grade_lower_bound = (model.pro_rec_rate/(metal_price-refining_cost)).min()\n",
    "    # col indexes\n",
    "    model.mb_cols = ['mineID', 'blockID']\n",
    "    model.dl_cols = ['dumpID', 'locationID']\n",
    "    model.mbs_cols = model.mb_cols + ['binID']\n",
    "    model.mbdl_cols = model.mb_cols + model.dl_cols\n",
    "    # data\n",
    "    mine_block = model.mine_block\n",
    "    dump_location = model.dump_location\n",
    "\n",
    "    # excavated\n",
    "    model.excavated = MakeData(model, mine_block[model.mb_cols], model.mb_cols).make_df(range(1, periods+1))\n",
    "    # filled\n",
    "    model.filled = MakeData(model, dump_location[model.dl_cols], model.dl_cols).make_df(range(1, periods+1))\n",
    "    # t_m_b_p\n",
    "    t_m_b = MakeData(model, mine_block[model.mb_cols], model.mb_cols).make_df(range(1, periods+1))\n",
    "    m_b_p = MakeData(model, mine_block.loc[mine_block['grade']>=grade_margin, \n",
    "                                           model.mb_cols+['grade', 'mining_cost', 'quantity']], model.mb_cols).make_df(\n",
    "        range(1, n_processors+1), ind='processorID')\n",
    "    t_m_b_p = MakeData(model, m_b_p, m_b_p.columns.tolist()).make_df(range(1, periods+1))\n",
    "    t_m_b_p = get_params(t_m_b_p, model.distance_m_b_p, params=['distance_m_b_p'], ind=model.distance_mbp_ind)\n",
    "    model.t_m_b_p = get_params(t_m_b_p, model.processor_recovery,\n",
    "                               params=['processing_cost', 'recovery_rate'], ind=model.mb_cols+['processorID'])\n",
    "    # selling_price_m_b_p =\n",
    "    # (metal_price-refinining_cost)*grade_m_b*recovery_m_b_p-mining_cost_m_b-haulage_cost_m_b_p-processing_cost_m_b_p\n",
    "    model.t_m_b_p['selling_price'] = np.where(model.t_m_b_p['processorID']==1,\n",
    "                                              (metal_price - refining_cost) * model.t_m_b_p.grade * model.t_m_b_p.recovery_rate - model.t_m_b_p.mining_cost - haulage_cost * model.t_m_b_p.distance_m_b_p - model.t_m_b_p.processing_cost,\n",
    "                                              (metal_price - refining_cost) * model.t_m_b_p.grade * model.t_m_b_p.recovery_rate - model.t_m_b_p.mining_cost - (haulage_cost * (model.t_m_b_p.distance_m_b_p-40) + train_haulage_cost * 40) - model.t_m_b_p.processing_cost\n",
    "                                              )\n",
    "    # t_m_b_s\n",
    "    t_m_b_s = MakeData(model, model.df_bins[model.mbs_cols+['density', 'if_active', 'quantity']],\n",
    "                       model.mbs_cols).make_df(range(1, periods+1))\n",
    "    t_m_b_s = get_params(t_m_b_s, model.distance_m_b_s, params=['distance_m_b_s'], ind=model.distance_mbs_ind)\n",
    "    # 14/09/2022 -- revised: replace with the real distance to bin[-999] if grade=0\n",
    "    '''t_m_b_s.distance_m_b_s = np.where(t_m_b_s.distance_m_b_s==0,\n",
    "                                      model.distance_m_b_s.loc[model.distance_m_b_s.binID==-999].distance_m_b_s.tolist()[0],\n",
    "                                      t_m_b_s.distance_m_b_s)'''\n",
    "    model.t_m_b_s = get_params(t_m_b_s, model.processor_recovery, params=['mining_cost', 'grade', 'avg_grade'],\n",
    "                               ind=model.mb_cols+['binID'])\n",
    "    # selling_price_m_b_s =\n",
    "    # -mining_cost_m_b-haulage_cost_m_b_s\n",
    "    # 05/10/2022 -- revised: if the grade is less than the avg_grade, reduce selling price; otherwise, increase selling price.\n",
    "    model.t_m_b_s['grade_diff'] = model.t_m_b_s['avg_grade'] - model.t_m_b_s['grade']\n",
    "    #model.t_m_b_s['selling_price'] = -model.t_m_b_s.mining_cost - haulage_cost * model.t_m_b_s.distance_m_b_s - (metal_price - refining_cost) * model.t_m_b_s.grade_diff * 0.008\n",
    "    model.t_m_b_s['selling_price'] = -model.t_m_b_s.mining_cost - haulage_cost * model.t_m_b_s.distance_m_b_s\n",
    "    # t_m_b_d_l\n",
    "    m_b_d_l_active = MakeData(model, mine_block.loc[(mine_block['if_active']==1) & \n",
    "                                                    (mine_block['grade']<round(model.grade_lower_bound+0.005,2)),\n",
    "                                                    model.mb_cols+['density', 'grade', 'wting', 'if_active', 'quantity']], \n",
    "                              model.mb_cols).make_df(\n",
    "        dump_location.loc[dump_location['fill_active']==1].dump_location.unique().tolist(), ind='dump_location')\n",
    "    m_b_d_l_nonactive = MakeData(model, mine_block.loc[(mine_block['if_active']==0) & \n",
    "                                                       (mine_block['grade']<round(model.grade_lower_bound+0.005,2)),\n",
    "                                                       model.mb_cols+['density', 'grade', 'wting', 'if_active', 'quantity']],\n",
    "                                 model.mb_cols).make_df(dump_location.dump_location.unique().tolist(), ind='dump_location')\n",
    "    m_b_d_l = pd.concat([m_b_d_l_active, m_b_d_l_nonactive])\n",
    "    m_b_d_l = get_params(m_b_d_l, dump_location, params=['fill_active'], ind=['dump_location'])\n",
    "    m_b_d_l = pd.concat([m_b_d_l.drop(columns='dump_location'),\n",
    "                         MakeData(model, m_b_d_l, 'dumpID').split_cols('locationID')], axis=1)\n",
    "    t_m_b_d_l = MakeData(model, m_b_d_l, m_b_d_l.columns.tolist()).make_df(range(1, periods+1))\n",
    "    t_m_b_d_l = get_params(t_m_b_d_l, model.distance_m_b_d_l, params=['distance_m_b_d_l'], ind=model.distance_mbdl_ind)\n",
    "    t_m_b_d_l = get_params(t_m_b_d_l, model.rou, params=['swell_factor'], ind=['wting'])\n",
    "    model.t_m_b_d_l = get_params(t_m_b_d_l, model.processor_recovery, params=['mining_cost'], ind=model.mb_cols)\n",
    "    # selling_price_m_b_d_l =\n",
    "    # -mining_cost_m_b-haulage_cost_m_b_d_l\n",
    "    # 04/10 -- revised incur additional fees when non-active to active locations\n",
    "#     model.t_m_b_d_l['selling_price'] = np.where((model.t_m_b_d_l['if_active']==0) & (model.t_m_b_d_l['fill_active']==1),\n",
    "#                                                 -model.t_m_b_d_l.mining_cost - haulage_cost * model.t_m_b_d_l.distance_m_b_d_l - 0.01,\n",
    "#                                                 np.where((model.t_m_b_d_l['if_active']==0) & (model.t_m_b_d_l['dumpID']==1),\n",
    "#                                                          -model.t_m_b_d_l.mining_cost - haulage_cost * model.t_m_b_d_l.distance_m_b_d_l - 0.3,\n",
    "#                                                          -model.t_m_b_d_l.mining_cost - haulage_cost * model.t_m_b_d_l.distance_m_b_d_l\n",
    "#                                                         )\n",
    "#                                                )\n",
    "    model.t_m_b_d_l['selling_price'] = -model.t_m_b_d_l.mining_cost - haulage_cost * model.t_m_b_d_l.distance_m_b_d_l\n",
    "    \n",
    "    # t_s_p\n",
    "    bin_p = pd.DataFrame(model.df_bins.loc[model.df_bins.binID>0].binID.unique().tolist(), columns=['binID'])\n",
    "    s_p = MakeData(model, bin_p, bin_p.columns.tolist()).make_df(range(1, n_processors+1), ind='processorID')\n",
    "    t_s_p = MakeData(model, s_p, s_p.columns.tolist()).make_df(range(1, periods+1))\n",
    "    t_s_p = get_params(t_s_p, model.distance_s_p, params=['distance_s_p'], ind=['processorID'])\n",
    "    # selling_price_s_p =\n",
    "    # (metal_price-refinining_cost)*grade_s*recovery_s_p-rehandling_cost-haulage_cost_s_p-processing_cost_s_p\n",
    "    model.t_s_p = get_params(t_s_p, model.processor_recovery, params=['avg_grade', 'processing_cost', 'recovery_rate'],\n",
    "                             ind=['binID', 'processorID'])\n",
    "    model.t_s_p['selling_price'] = np.where(model.t_s_p['processorID']==1,\n",
    "                                            (metal_price - refining_cost) * model.t_s_p.avg_grade * model.t_s_p.recovery_rate - rehandling_cost - haulage_cost * model.t_s_p.distance_s_p - model.t_s_p.processing_cost,\n",
    "                                            (metal_price - refining_cost) * model.t_s_p.avg_grade * model.t_s_p.recovery_rate - rehandling_cost - (haulage_cost * (model.t_s_p.distance_s_p-40) + train_haulage_cost * 40) - model.t_s_p.processing_cost\n",
    "                                            )\n",
    "    # t_s_d_l\n",
    "    if grade_zero:\n",
    "        bin_d = pd.DataFrame(np.array([[0, 1, 1.7]]), columns=['binID', 'wting', 'density'])\n",
    "    else:\n",
    "        bin_d = model.df_bins.loc[model.df_bins['binID'].isin([-999,-998,-997]),\n",
    "                                  ['binID', 'wting', 'density']].drop_duplicates()\n",
    "    \n",
    "    s_dl = MakeData(model, bin_d, bin_d.columns.tolist()).make_df(dump_location.loc[dump_location.dumpID==2].dump_location.unique().tolist(),\n",
    "                                                                 ind='dump_location')\n",
    "    s_d_l = get_params(s_dl, model.dump_location, params=['fill_active'], ind=['dump_location'])\n",
    "    s_d_l = pd.concat([s_dl.drop(columns='dump_location'), MakeData(model, s_dl, 'dumpID').split_cols('locationID')], axis=1)\n",
    "\n",
    "#     s_d_l = pd.concat([s_dl.drop(columns='dump_location'),\n",
    "#                        MakeData(model, s_dl, 'dumpID').split_cols('locationID')], axis=1)\n",
    "    t_s_d_l = MakeData(model, s_d_l, s_d_l.columns.tolist()).make_df(range(1, periods+1))\n",
    "    t_s_d_l = get_params(t_s_d_l, rou, params=['swell_factor'], ind=['wting'])\n",
    "\n",
    "    '''\n",
    "    # 31/10 -- revised add new columns 'swelled_density' to eliminate duplication and delete original columns 'swell_factor'\n",
    "    # & 'density'\n",
    "    t_s_d_l['swelled_density'] = t_s_d_l['density'] / t_s_d_l['swell_factor']\n",
    "    t_s_d_l.drop(['density', 'swell_factor'], axis=1, inplace=True)\n",
    "    t_s_d_l.drop_duplicates(inplace=True)\n",
    "    '''\n",
    "    model.t_s_d_l = get_params(t_s_d_l, model.distance_s_d_l, params=['distance_s_d_l'], ind=model.distance_sdl_ind)\n",
    "    # selling_price_s_d_l =\n",
    "    # -rehandling_cost-haulage_cost_s_d_l\n",
    "    model.t_s_d_l['selling_price'] = -rehandling_cost - haulage_cost * model.t_s_d_l.distance_s_d_l\n",
    "\n",
    "    # add name for each dataframe\n",
    "    model.excavated['name'] = model.excavated.apply(lambda x: \"excavated_time%d_mine%d_block%d\" %\n",
    "                                                              (x['time_period'], x['mineID'], x['blockID']), axis=1)\n",
    "    model.filled['name'] = model.filled.apply(lambda x: \"filled_time%d_dump%d_loc%d\" %\n",
    "                                              (x['time_period'], x['dumpID'], x['locationID']), axis=1)\n",
    "    model.t_m_b_p['name'] = model.t_m_b_p.apply(lambda x: \"time%d_mine%d_block%d_processor%d\" %\n",
    "                                                (x['time_period'], x['mineID'], x['blockID'], x['processorID']), axis=1)\n",
    "    model.t_m_b_s['name'] = model.t_m_b_s.apply(lambda x: \"time%d_mine%d_block%d_bin%d\" %\n",
    "                                                (x.time_period, x.mineID, x.blockID, x.binID), axis=1)\n",
    "    model.t_m_b_d_l['name'] = model.t_m_b_d_l.apply(lambda x: \"time%d_mine%d_block%d_dump%d_loc%d\" %\n",
    "                                                    (x.time_period, x.mineID, x.blockID, x.dumpID, x.locationID), axis=1)\n",
    "    model.t_s_p['name'] = model.t_s_p.apply(lambda x: \"time%d_bin%d_processor%d\" %\n",
    "                                            (x.time_period, x.binID, x.processorID), axis=1)\n",
    "    model.t_s_d_l['name'] = model.t_s_d_l.apply(lambda x: \"time%d_bin%d_dump%d_den%.2f_loc%d\" %\n",
    "                                                (x.time_period, x.binID, x.dumpID, x.density, x.locationID), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba8f7345",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_variables(model):\n",
    "    discount = model.other_params['discount_rate']\n",
    "    # auxiliary function to create variables from a row\n",
    "    def create_var(model, df, var_name, var_type='cont'):\n",
    "        df.set_index('name', inplace=True)\n",
    "        if var_type=='cont':\n",
    "            df[var_name] = df.apply(lambda r: model.continuous_var(name=\"tons_%s\" % r.name, lb=0), axis=1)\n",
    "            # make_cont_var(r, \"tons_%s\"), axis=1)\n",
    "        else:\n",
    "            df[var_name] = df.apply(lambda r: model.binary_var(name=\"%s\" % r.name), axis=1)\n",
    "            # make_cat_var(r, \"%s\"), axis=1)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        return df\n",
    "\n",
    "    # excavated\n",
    "    model.excavated = create_var(model, model.excavated, \"excavated\", var_type='cat')\n",
    "    model.excavated['sum_excavated'] = model.excavated.groupby(['mineID', 'blockID']).excavated.apply(lambda x: x.cumsum())\n",
    "    # filled\n",
    "    model.filled = create_var(model, model.filled, \"filled\", var_type='cat')\n",
    "    # t_m_b_p\n",
    "    model.t_m_b_p = create_var(model, model.t_m_b_p, \"tons_t_m_b_p\")\n",
    "    model.t_m_b_p['profit'] = model.t_m_b_p.tons_t_m_b_p * model.t_m_b_p.selling_price / ((1 + discount/100)**model.t_m_b_p.time_period)\n",
    "    # t_m_b_s\n",
    "    model.t_m_b_s = create_var(model, model.t_m_b_s, \"tons_t_m_b_s\")\n",
    "    model.t_m_b_s['sum_m_b_s'] = model.t_m_b_s.groupby(['mineID', 'blockID']).tons_t_m_b_s.apply(lambda x: x.cumsum())\n",
    "    model.t_m_b_s['profit'] = model.t_m_b_s.tons_t_m_b_s * model.t_m_b_s.selling_price / ((1 + discount/100)**model.t_m_b_s.time_period)\n",
    "    # t_m_b_d_l\n",
    "    model.t_m_b_d_l = create_var(model, model.t_m_b_d_l, \"tons_t_m_b_d_l\")\n",
    "    model.t_m_b_d_l['vol_t_m_b_d_l'] = (model.t_m_b_d_l['swell_factor']*model.t_m_b_d_l['tons_t_m_b_d_l'])/model.t_m_b_d_l['density']\n",
    "    model.t_m_b_d_l['sum_m_b_filled'] = model.t_m_b_d_l.groupby(['mineID', 'blockID', 'dumpID']).vol_t_m_b_d_l.apply(\n",
    "        lambda x: x.cumsum())\n",
    "    model.t_m_b_d_l['profit'] = model.t_m_b_d_l.tons_t_m_b_d_l * model.t_m_b_d_l.selling_price / ((1 + discount/100)**model.t_m_b_d_l.time_period)\n",
    "    # t_s_p\n",
    "    model.t_s_p = create_var(model, model.t_s_p, \"tons_t_s_p\")\n",
    "    model.t_s_p['sum_s_p'] = model.t_s_p.groupby(['binID', 'processorID']).tons_t_s_p.apply(lambda x: x.cumsum())\n",
    "    model.t_s_p['profit'] = model.t_s_p.tons_t_s_p * model.t_s_p.selling_price / ((1 + discount/100)**model.t_s_p.time_period)\n",
    "    # t_s_d_l\n",
    "    model.t_s_d_l = create_var(model, model.t_s_d_l, \"tons_t_s_d_l\")\n",
    "    model.t_s_d_l['vol_t_s_d_l'] = (model.t_s_d_l['swell_factor'] * model.t_s_d_l['tons_t_s_d_l']) / model.t_s_d_l['density']\n",
    "    model.t_s_d_l['profit'] = model.t_s_d_l.tons_t_s_d_l * model.t_s_d_l.selling_price / ((1 + discount/100)**model.t_s_d_l.time_period)\n",
    "    model.t_s_d_l = model.t_s_d_l.groupby(['binID', 'dumpID', 'locationID', 'time_period', 'fill_active', 'distance_s_d_l', \n",
    "                                           'selling_price'])['tons_t_s_d_l', 'vol_t_s_d_l', 'profit'].sum().reset_index()\n",
    "    model.t_s_d_l['sum_s_filled'] = model.t_s_d_l.groupby(['binID', 'dumpID', 'locationID']).vol_t_s_d_l.apply(lambda x: \n",
    "                                                                                                               x.cumsum())\n",
    "    model.t_s_d_l['sum_s_d_l'] = model.t_s_d_l.groupby(['binID', 'dumpID', 'locationID']).tons_t_s_d_l.apply(lambda x: \n",
    "                                                                                                             x.cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c455f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constraints -> mdl.add_constraint\n",
    "def setup_constraints(model):\n",
    "    periods = int(model.other_params['time period'])\n",
    "    grade_lower_bound = model.grade_lower_bound\n",
    "    mine_block = model.mine_block\n",
    "    dump_location = model.dump_location\n",
    "    #dumping_priority = model.dumping_priority\n",
    "\n",
    "    excavated = model.excavated\n",
    "    filled = model.filled\n",
    "    t_m_b_p = model.t_m_b_p\n",
    "    t_m_b_s = model.t_m_b_s\n",
    "    t_m_b_d_l = model.t_m_b_d_l\n",
    "    t_s_p = model.t_s_p\n",
    "    t_s_d_l = model.t_s_d_l\n",
    "\n",
    "    def pin_period(df, col, v):\n",
    "        cond = (df[col] <= v)\n",
    "        return cond\n",
    "\n",
    "    def precedences(model, df, col, const=['time_period'], ls=list()):\n",
    "        IDs = []\n",
    "        ts = []\n",
    "        values = []\n",
    "        col_name = ls + const + [col]\n",
    "        ls_len = len(ls)\n",
    "        for item, item_flow in df.groupby(level=ls):\n",
    "            for t in range(1, periods+1):\n",
    "                value = model.sum(item_flow[pin_period(item_flow, 'time_period', t)][col])\n",
    "                if type(item) != tuple:\n",
    "                    ID = item\n",
    "                else:\n",
    "                    ID = list(item)\n",
    "                IDs.append(ID)\n",
    "                ts.append(t)\n",
    "                values.append(value)\n",
    "\n",
    "        trans = np.ravel(IDs, order='F').reshape(ls_len,-1).T\n",
    "        values = np.array(values).reshape(-1,1)\n",
    "        ts = np.array(ts).reshape(-1,1)\n",
    "        merged = np.concatenate((trans, ts, values), axis=1)\n",
    "        df_final = pd.DataFrame(merged, columns=col_name)\n",
    "        df_final.rename(columns={col_name[-1]:col_name[-1].replace(col_name[-1][: col_name[-1].index('_')], 'sum')},\n",
    "                        inplace=True)\n",
    "        return df_final\n",
    "\n",
    "    # rule: active waste rocks don't go to the waste stockpile bin\n",
    "    # 30/08/2022 -- revised: no constraint if there is no block with grade 0\n",
    "    number_of_active_constraints = 0\n",
    "    for r in t_m_b_s.itertuples():\n",
    "        if len(model.df_bins.loc[model.df_bins.avg_grade==0].index.tolist()) == 0:\n",
    "            pass\n",
    "        elif r.if_active == 1 and r.binID == 0:\n",
    "            model.add_constraint(r.tons_t_m_b_s == 0)\n",
    "            number_of_active_constraints += 1\n",
    "    print(\"#waste stockpile non-active constraints: {}\".format(number_of_active_constraints))\n",
    "    # model.print_information()\n",
    "    # 1.1 reserve constraint\n",
    "    number_of_reserve_constraints = 0\n",
    "    reserve_level_cols = ['mineID', 'blockID', 'time_period']\n",
    "    r1 = MakeData(model, t_m_b_p.set_index(reserve_level_cols), 'tons_t_m_b_p').make_params(reserve_level_cols)\n",
    "    r2 = MakeData(model, t_m_b_s.set_index(reserve_level_cols), 'tons_t_m_b_s').make_params(reserve_level_cols)\n",
    "    r3 = MakeData(model, t_m_b_d_l.set_index(reserve_level_cols), 'tons_t_m_b_d_l').make_params(reserve_level_cols)\n",
    "    # reserve = get_params(reserve, excavated, params=['excavated'], ind=)\n",
    "    reserve = get_params(excavated[['time_period', 'mineID', 'blockID', 'excavated']], \n",
    "                         mine_block[['mineID', 'blockID', 'quantity']], \n",
    "                         params=['quantity'], ind=['mineID', 'blockID'], dropped=False)\n",
    "    reserve = get_params(reserve, r1, params=['sum_t_m_b_p'], ind=reserve_level_cols, dropped=False)\n",
    "    reserve = get_params(reserve, r2, params=['sum_t_m_b_s'], ind=reserve_level_cols, dropped=False)\n",
    "    reserve = get_params(reserve, r3, params=['sum_t_m_b_d_l'], ind=reserve_level_cols, dropped=False)\n",
    "\n",
    "    for r in reserve.itertuples():\n",
    "        model.add_constraint(r.sum_t_m_b_p + r.sum_t_m_b_s + r.sum_t_m_b_d_l == r.excavated * r.quantity)\n",
    "        number_of_reserve_constraints += 1\n",
    "    print(\"#reserve constraints: {}\".format(number_of_reserve_constraints))\n",
    "    # model.print_information()\n",
    "    # 1.2 only-one-period mining\n",
    "    for e in excavated.loc[excavated.time_period==periods].itertuples():\n",
    "        model.add_constraint(e.sum_excavated <= 1)\n",
    "    # 2. mining capacity\n",
    "    number_of_miningCapacity_constraints = 0\n",
    "    #mining_level_cols = ['mineID', 'time_period']\n",
    "#     mining_level_cols = ['time_period']\n",
    "#     mc1 = MakeData(model, t_m_b_p.set_index(mining_level_cols), 'tons_t_m_b_p').make_params(mining_level_cols)\n",
    "#     mc2 = MakeData(model, t_m_b_s.set_index(mining_level_cols), 'tons_t_m_b_s').make_params(mining_level_cols)\n",
    "#     mc3 = MakeData(model, t_m_b_d_l.set_index(mining_level_cols), 'tons_t_m_b_d_l').make_params(mining_level_cols)\n",
    "\n",
    "#     mineCap = mc1.join(mc2.sum_t_m_b_s).join(mc3.sum_t_m_b_d_l)\n",
    "    mineCap = excavated[['time_period', 'mineID', 'blockID', 'excavated']]\n",
    "    mineCap = get_params(mineCap, mine_block, params=['quantity'], ind=['mineID', 'blockID'])\n",
    "    mineCap['excavated_quantity'] = mineCap['excavated'] * mineCap['quantity']\n",
    "    mine_params = ['lower_bound', 'upper_bound']\n",
    "    #mine_ind = ['mineID']\n",
    "    #mineCap = get_params(mineCap, model.mining_cap, params=mine_params, ind=mine_ind)\n",
    "    mineCap['lower_bound'] = 0\n",
    "    mineCap['upper_bound'] = 950000\n",
    "    mineCap = MakeData(model, mineCap.set_index(['time_period']+mine_params), 'excavated_quantity').make_params(\n",
    "        ['time_period']+mine_params)\n",
    "\n",
    "    for m in mineCap.itertuples():\n",
    "        if m.time_period < periods:\n",
    "            model.add_constraint(m.sum_quantity <= m.upper_bound)\n",
    "            model.add_constraint(m.sum_quantity >= m.lower_bound)\n",
    "            number_of_miningCapacity_constraints += 2\n",
    "        else:\n",
    "            model.add_constraint(m.sum_quantity <= m.upper_bound)\n",
    "            number_of_miningCapacity_constraints += 1\n",
    "    print(\"#mining capacity constraints: {}\".format(number_of_miningCapacity_constraints))\n",
    "    # model.print_information()\n",
    "    # 3. processing capacity\n",
    "    number_of_processorCapacity_constraints = 0\n",
    "    bin_ind = ['binID']\n",
    "    processing_level_cols = ['processorID', 'time_period']\n",
    "    pc1 = MakeData(model, t_m_b_p.set_index(processing_level_cols), 'tons_t_m_b_p').make_params(processing_level_cols)\n",
    "    pc2 = MakeData(model, t_s_p.set_index(processing_level_cols), 'tons_t_s_p').make_params(processing_level_cols)\n",
    "    processorCap = pc1.join(pc2.sum_t_s_p)\n",
    "    # processorCap = reserve[['']]\n",
    "\n",
    "    processor_params = ['lower_bound', 'upper_bound']\n",
    "    processor_ind = ['processorID']\n",
    "    processorCap = get_params(processorCap, model.processing_cap, params=processor_params, ind=processor_ind)\n",
    "\n",
    "    for p in processorCap.itertuples():\n",
    "        if p.time_period < periods:\n",
    "            model.add_constraint(p.sum_t_m_b_p + p.sum_t_s_p <= p.upper_bound)\n",
    "            model.add_constraint(p.sum_t_m_b_p + p.sum_t_s_p >= p.lower_bound)\n",
    "            number_of_processorCapacity_constraints += 2\n",
    "        else:\n",
    "            model.add_constraint(p.sum_t_m_b_p + p.sum_t_s_p <= p.upper_bound)\n",
    "            number_of_processorCapacity_constraints += 1\n",
    "    print(\"#processing capacity constraints: {}\".format(number_of_processorCapacity_constraints))\n",
    "    # model.print_information()\n",
    "    # 4. dumping capacity\n",
    "    number_of_dumpLocCapacity_constraints = 0\n",
    "    dumping_level_cols = ['dumpID', 'locationID']\n",
    "    dlc1 = MakeData(model, t_m_b_d_l.set_index(dumping_level_cols), 'vol_t_m_b_d_l').make_params(dumping_level_cols)\n",
    "    dlc2 = MakeData(model, t_s_d_l.set_index(dumping_level_cols), 'vol_t_s_d_l').make_params(dumping_level_cols)\n",
    "    dumpLocCap = dlc1.join(dlc2.sum_t_s_d_l)\n",
    "\n",
    "    dumpLoc_params = ['volume']\n",
    "    dumpLoc_ind = ['dumpID', 'locationID']\n",
    "    #dump_vol = pd.DataFrame(model.df_dumps.groupby('dumpID').volume.sum().reset_index())\n",
    "    dumpLocCap = get_params(dumpLocCap, model.df_dumps, params=dumpLoc_params, ind=dumpLoc_ind)\n",
    "    ## =========================================================================================================\n",
    "    ## except the waste bin, other bins cannot go to dump????? -- to be decided by more reasonable cutoff grade\n",
    "    ## =========================================================================================================\n",
    "    for dl in dumpLocCap.itertuples():\n",
    "        if dl.dumpID == 2:\n",
    "            #model.add_constraint(dl.sum_t_m_b_d_l + dl.sum_t_s_d_l >= dl.volume - 10000) \n",
    "            model.add_constraint(dl.sum_t_m_b_d_l + dl.sum_t_s_d_l <= dl.volume) \n",
    "        else:\n",
    "            model.add_constraint(dl.sum_t_m_b_d_l + dl.sum_t_s_d_l <= dl.volume)\n",
    "        # model.add_constraint(dl.sum_t_m_b_d_l + dl.sum_t_s_d_l <= dl.volume)\n",
    "        number_of_dumpLocCapacity_constraints += 1\n",
    "    print(\"#dumping capacity constraints: {}\".format(number_of_dumpLocCapacity_constraints))\n",
    "    # model.print_information()\n",
    "    # 5. mining precedence\n",
    "    number_of_blockPrecedent_constraints = 0\n",
    "    block_level_cols = ['mineID', 'blockID']\n",
    "    blockPred_level_cols = ['mineID', 'precedenceID']\n",
    "\n",
    "    blockPre = pd.merge(model.df_block_precedence, excavated, \n",
    "                        left_on=blockPred_level_cols, right_on=block_level_cols)\n",
    "    blockPre = blockPre.drop(columns=['blockID_y', 'excavated'])\n",
    "    blockPre.rename(columns={'blockID_x': 'blockID', 'sum_excavated':'pre_sum_excavated'}, inplace=True)\n",
    "    blockPre = pd.merge(blockPre, excavated, on=block_level_cols+['time_period'])\n",
    "\n",
    "    for block_flow in blockPre.itertuples():\n",
    "        model.add_constraint(block_flow.excavated <= block_flow.pre_sum_excavated)\n",
    "        number_of_blockPrecedent_constraints += 1\n",
    "    print(\"#block precedence constraints: {}\".format(number_of_blockPrecedent_constraints))\n",
    "    # model.print_information()\n",
    "    # 6. dump location precedence\n",
    "    # 6.1 fill the current dump location if its precedent dump locations are all fully filled\n",
    "    number_of_dumpLocPrecedent_constraints = 0\n",
    "    dumpLocPred_level_cols = ['dumpID', 'locationID']\n",
    "    dumpLocPre_t_m_b_d_l = t_m_b_d_l.rename(columns={'locationID':'precedenceID'})\n",
    "    dumpLocPre_t_s_d_l = t_s_d_l.rename(columns={'locationID':'precedenceID'})\n",
    "    dumpLocPre_t_m_b_d_l = get_params(dumpLocPre_t_m_b_d_l, model.df_waste_precedence,\n",
    "                                      params=model.dl_pre_cols[-1:], ind=model.dl_pre_cols[:-1], repetitive=True, dropped=True)\n",
    "    dumpLocPre_t_s_d_l = get_params(dumpLocPre_t_s_d_l, model.df_waste_precedence,\n",
    "                                    params=model.dl_pre_cols[-1:], ind=model.dl_pre_cols[:-1], repetitive=True, dropped=True)\n",
    "    dumpLocPre_m_b = precedences(model, dumpLocPre_t_m_b_d_l.set_index(model.dl_pre_cols),\n",
    "                                 col='vol_t_m_b_d_l', ls=dumpLocPred_level_cols)\n",
    "    dumpLocPre_s = precedences(model, dumpLocPre_t_s_d_l.set_index(model.dl_pre_cols),\n",
    "                               col='vol_t_s_d_l', ls=dumpLocPred_level_cols)\n",
    "    dumpLocPre = dumpLocPre_m_b.join(dumpLocPre_s.sum_t_s_d_l)\n",
    "    dumpLocPre.dumpID = dumpLocPre.dumpID.astype(int)\n",
    "    dumpLocPre.locationID = dumpLocPre.locationID.astype(int)\n",
    "    # get precedent volume\n",
    "    dumpLocPre_volume = model.df_waste_precedence.groupby(by=dumpLocPred_level_cols)['volume'].sum().reset_index()\n",
    "    dumpLocPre_volume = pd.merge(dumpLocPre_volume, filled, on=dumpLocPred_level_cols)\n",
    "\n",
    "    for dumpLoc_flow in dumpLocPre.join(dumpLocPre_volume[['volume', 'filled']]).itertuples():\n",
    "        model.add_constraint(dumpLoc_flow.sum_t_m_b_d_l + dumpLoc_flow.sum_t_s_d_l >=\n",
    "                             dumpLoc_flow.volume * dumpLoc_flow.filled)\n",
    "        number_of_dumpLocPrecedent_constraints += 1\n",
    "    print(\"#dump location precedence constraints: {}\".format(number_of_dumpLocPrecedent_constraints))\n",
    "    # model.print_information()\n",
    "    # 6.2 filling capacity of the current dump location should not exceed its volume == dumping capacity satisfied\n",
    "    #     in each time period\n",
    "    number_of_dumpLocNow_constraints = 0\n",
    "    dumpLocNow_m_b = precedences(model, t_m_b_d_l.set_index(dumpLocPred_level_cols+['mineID', 'blockID']),\n",
    "                                 col='vol_t_m_b_d_l', ls=dumpLocPred_level_cols)\n",
    "    dumpLocNow_s = precedences(model, t_s_d_l.set_index(dumpLocPred_level_cols+bin_ind), col='vol_t_s_d_l',\n",
    "                               ls=dumpLocPred_level_cols)\n",
    "    ####### running slow -- needs to be modified!!! => pd.concat()\n",
    "    dumpLocNow = dumpLocNow_m_b.join(dumpLocNow_s.sum_t_s_d_l)\n",
    "    dumpLocNow = get_params(dumpLocNow, model.df_dumps, params=['volume'], ind=dumpLocPred_level_cols)\n",
    "    dumpLocNow = get_params(dumpLocNow, filled, params=['filled'], ind=dumpLocPred_level_cols+['time_period'])\n",
    "\n",
    "    for dumpLoc_flow in dumpLocNow.itertuples():\n",
    "        model.add_constraint(dumpLoc_flow.sum_t_m_b_d_l + dumpLoc_flow.sum_t_s_d_l <=\n",
    "                             dumpLoc_flow.volume * dumpLoc_flow.filled)\n",
    "        number_of_dumpLocNow_constraints += 1\n",
    "    print(\"#dump location now constraints: {}\".format(number_of_dumpLocNow_constraints))\n",
    "    model.print_information()\n",
    "#     7. dump location priority\n",
    "#     fill the current dump location if its previous dump locations are all fully filled\n",
    "#     number_of_dumpLocPriority_constraints = 0\n",
    "#     dumpLocPred_level_cols = ['dumpID', 'locationID']\n",
    "#     dumpLocPre_t_m_b_d_l = t_m_b_d_l.rename(columns={'locationID':'precedenceID'})\n",
    "#     dumpLocPre_t_m_b_d_l = get_params(dumpLocPre_t_m_b_d_l, dumping_priority,\n",
    "#                                       params=model.dl_pre_cols[-1:], ind=model.dl_pre_cols[:-1], repetitive=True, dropped=True)\n",
    "#     dumpLocPre = precedences(model, dumpLocPre_t_m_b_d_l.set_index(model.dl_pre_cols),\n",
    "#                              col='density_t_m_b_d_l', ls=dumpLocPred_level_cols)\n",
    "#     dumpLocPre.dumpID = dumpLocPre.dumpID.astype(int)\n",
    "#     dumpLocPre.locationID = dumpLocPre.locationID.astype(int)\n",
    "#     # get precedent volume\n",
    "#     dumpLocPre_volume = dumping_priority.groupby(by=dumpLocPred_level_cols)['volume'].sum().reset_index()\n",
    "#     dumpLocPre_volume = pd.merge(dumpLocPre_volume, filled, on=dumpLocPred_level_cols)\n",
    "\n",
    "#     for dumpLoc_flow in dumpLocPre.join(dumpLocPre_volume[['volume', 'filled']]).itertuples():\n",
    "#         model.add_constraint(dumpLoc_flow.sum_t_m_b_d_l >= dumpLoc_flow.volume * dumpLoc_flow.filled)\n",
    "#         number_of_dumpLocPriority_constraints += 1\n",
    "#     print(\"#dump location priority constraints: {}\".format(number_of_dumpLocPriority_constraints))\n",
    "    # 8. stockpile capacity\n",
    "    number_of_binCapacity_constraints = 0\n",
    "    bin_level_cols = ['binID']\n",
    "    bin_params = ['bin_capacity']\n",
    "    bin_m_b = precedences(model, t_m_b_s.set_index(model.mb_cols+bin_ind), col='tons_t_m_b_s', ls=bin_level_cols)\n",
    "    bin_s_p = precedences(model, t_s_p.set_index(bin_ind+processor_ind), col='tons_t_s_p', ls=bin_level_cols)\n",
    "    bin_s_d = precedences(model, t_s_d_l.set_index(bin_ind+dumpLoc_ind), col='tons_t_s_d_l', ls=bin_level_cols)\n",
    "    binCap = bin_m_b.merge(bin_s_p, how='left').merge(bin_s_d, how='left')\n",
    "    binCap = get_params(binCap, model.bin_capacity_grade, params=bin_params, ind=bin_ind)     \n",
    "\n",
    "    for bin_flow in binCap.itertuples():\n",
    "        '''\n",
    "        if bin_flow.time_period == 1:\n",
    "            model.add_constraint(bin_flow.sum_t_s_p + bin_flow.sum_t_s_d_l == 0)\n",
    "        '''\n",
    "        model.add_constraint(bin_flow.sum_t_m_b_s - bin_flow.sum_t_s_p - bin_flow.sum_t_s_d_l >= 0)\n",
    "        model.add_constraint(bin_flow.sum_t_m_b_s - bin_flow.sum_t_s_p - bin_flow.sum_t_s_d_l <= bin_flow.bin_capacity)\n",
    "        number_of_binCapacity_constraints += 2\n",
    "    print(\"#Stockpile bin capacity constraints: {}\".format(number_of_binCapacity_constraints))\n",
    "\n",
    "    # 16/09/2022 -- revised: no material flow from stockpiles when time period = 1\n",
    "    number_of_voidBin_constraints = 0\n",
    "    bin_flow_year0 = binCap.loc[binCap.time_period==1]\n",
    "    for bin_flow in bin_flow_year0.itertuples():\n",
    "        model.add_constraint(bin_flow.sum_t_s_p + bin_flow.sum_t_s_d_l == 0)\n",
    "        number_of_voidBin_constraints += 1\n",
    "    print(\"#Stockpile bin void flow constraints: {}\".format(number_of_voidBin_constraints))\n",
    "    # model.print_information()\n",
    "    # 9. grade blending\n",
    "    number_of_gradeBlend_constraints = 0\n",
    "    gradeBlend = pd.concat([t_m_b_p[processing_level_cols+['grade', 'tons_t_m_b_p']].rename(columns={'tons_t_m_b_p': 'tons'}),\n",
    "                            t_s_p[processing_level_cols+['avg_grade', 'tons_t_s_p']].rename(columns={'tons_t_s_p': 'tons',\n",
    "                                                                                                     'avg_grade': 'grade'})],\n",
    "                           ignore_index=True)\n",
    "    gradeBlend['grade_tons'] = gradeBlend['grade'] * gradeBlend['tons']\n",
    "\n",
    "    for p, grade in gradeBlend.groupby(by=processing_level_cols):\n",
    "        model.add_constraint(model.sum(grade.grade_tons) >= grade_lower_bound * model.sum(grade.tons))\n",
    "        number_of_gradeBlend_constraints += 1\n",
    "    print(\"#Grade blending constraints: {}\".format(number_of_gradeBlend_constraints))\n",
    "\n",
    "    model.print_information()\n",
    "\n",
    "\n",
    "def setup_objective(model, data_url):\n",
    "    total_t_m_b_p_revenues = model.sum(model.t_m_b_p['profit'])\n",
    "    total_t_m_b_s_revenues = model.sum(model.t_m_b_s['profit'])\n",
    "    total_t_m_b_d_l_revenues = model.sum(model.t_m_b_d_l['profit'])\n",
    "    total_t_s_p_revenues = model.sum(model.t_s_p['profit'])\n",
    "    total_t_s_d_l_revenues = model.sum(model.t_s_d_l['profit'])\n",
    "    model.add_kpi(total_t_m_b_p_revenues, \"Total t_m_b_p revenues\")\n",
    "    model.add_kpi(total_t_m_b_s_revenues, \"Total t_m_b_s revenues\")\n",
    "    model.add_kpi(total_t_m_b_d_l_revenues, \"Total t_m_b_d_l revenues\")\n",
    "    model.add_kpi(total_t_s_p_revenues, \"Total t_s_p revenues\")\n",
    "    model.add_kpi(total_t_s_d_l_revenues, \"Total t_s_d_l revenues\")\n",
    "\n",
    "    model.maximize(total_t_m_b_p_revenues + total_t_m_b_s_revenues + total_t_m_b_d_l_revenues +\n",
    "                   total_t_s_p_revenues + total_t_s_d_l_revenues)\n",
    "    # save the lp model\n",
    "    #model.export_as_lp(data_url+\"final_model_ji.lp\")\n",
    "    #model.export_as_lp(\"C:/Users/20937337/Desktop/final_model.lp\")\n",
    "\n",
    "\n",
    "def print_information(model):\n",
    "    model.print_information()\n",
    "    model.report_kpis()\n",
    "\n",
    "def solve(model, **kwargs):\n",
    "    # Here, we set the number of threads for CPLEX to 2 and set the time limit to 2mins.\n",
    "    #model.parameters.threads = 2\n",
    "    #model.parameters.timelimit = 120\n",
    "    model.parameters.mip.tolerances.mipgap.set(float(0.1))\n",
    "    model.parameters.mip.strategy.file.set(3)\n",
    "    sol = model.solve(log_output=True, SearchType='DepthFirst',**kwargs)\n",
    "    if sol is not None:\n",
    "        print(\"solution for a cost of {}\".format(model.objective_value))\n",
    "        #print_information(model)\n",
    "        #print_solution(model)\n",
    "        return sol, model.objective_value\n",
    "    else:\n",
    "        print(\"* model is infeasible\")\n",
    "        return None\n",
    "\n",
    "def get_solution(data, sol, var, trans=False):\n",
    "    data['var_value'] = sol.get_values(data[var])\n",
    "    if trans:\n",
    "        data['var_value'] = data['var_value'].astype(int)\n",
    "    data['var_value'] = data['var_value'].apply(lambda x: 0 if x < 1e-5 else x)\n",
    "    return data\n",
    "\n",
    "\n",
    "def build(bin_num, data_url, grade_margin, context=None, verbose=False, **kwargs):\n",
    "    #-----start------\n",
    "    mdl = Model(\"mine_prod\", context=context, **kwargs)\n",
    "    loading_t0 = datetime.now()\n",
    "    load_data(mdl, data_url, bin_num, grade_margin)\n",
    "    #print('loading data completed!')\n",
    "    #return mdl, loading_t0\n",
    "\n",
    "    setup_data(mdl)\n",
    "\n",
    "    preprocessing_t1 = datetime.now()\n",
    "    print(\"data loading time: \", (preprocessing_t1 - loading_t0).seconds/60)\n",
    "    setup_variables(mdl)\n",
    "    variables_setting_t2 = datetime.now()\n",
    "    print(\"data preprocessing time: \", (variables_setting_t2 - preprocessing_t1).seconds/60)\n",
    "    setup_constraints(mdl)\n",
    "    constraints_setting_t3 = datetime.now()\n",
    "    print(\"variables setting time: \", (constraints_setting_t3 - variables_setting_t2).seconds/60)\n",
    "    setup_objective(mdl, data_url)\n",
    "    objective_setting_t4 = datetime.now()\n",
    "    print(\"objective setting time: \", (objective_setting_t4 - constraints_setting_t3).seconds/60)\n",
    "    print(\"model completion time: \", (objective_setting_t4 - loading_t0).seconds/60)\n",
    "\n",
    "    return mdl, loading_t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5111192e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loading time:  0.5166666666666667\n",
      "data preprocessing time:  0.7666666666666667\n",
      "#waste stockpile non-active constraints: 0\n",
      "#reserve constraints: 8168\n",
      "#mining capacity constraints: 7\n",
      "#processing capacity constraints: 14\n",
      "#dumping capacity constraints: 101\n",
      "#block precedence constraints: 30340\n",
      "#dump location precedence constraints: 144\n",
      "#dump location now constraints: 404\n",
      "Model: mine_prod\n",
      " - number of variables: 548364\n",
      "   - binary=8572, integer=0, continuous=539792\n",
      " - number of constraints: 41220\n",
      "   - linear=41220\n",
      " - parameters: defaults\n",
      " - objective: none\n",
      " - problem type is: MILP\n",
      "#Stockpile bin capacity constraints: 632\n",
      "#Stockpile bin void flow constraints: 79\n",
      "#Grade blending constraints: 8\n",
      "Model: mine_prod\n",
      " - number of variables: 548364\n",
      "   - binary=8572, integer=0, continuous=539792\n",
      " - number of constraints: 41939\n",
      "   - linear=41939\n",
      " - parameters: defaults\n",
      " - objective: none\n",
      " - problem type is: MILP\n",
      "variables setting time:  0.4\n",
      "objective setting time:  0.016666666666666666\n",
      "model completion time:  1.7333333333333334\n",
      "Version identifier: 22.1.0.0 | 2022-03-09 | 1a383f8ce\n",
      "CPXPARAM_Read_DataCheck                          1\n",
      "CPXPARAM_MIP_Strategy_File                       3\n",
      "CPXPARAM_MIP_Tolerances_MIPGap                   0.10000000000000001\n",
      "Tried aggregator 1 time.\n",
      "MIP Presolve eliminated 535 rows and 614 columns.\n",
      "MIP Presolve modified 4 coefficients.\n",
      "Reduced MIP has 41404 rows, 547750 columns, and 5815514 nonzeros.\n",
      "Reduced MIP has 8312 binaries, 0 generals, 0 SOSs, and 0 indicators.\n",
      "Presolve time = 2.70 sec. (2004.07 ticks)\n",
      "Tried aggregator 1 time.\n",
      "Detecting symmetries...\n",
      "Reduced MIP has 41404 rows, 547750 columns, and 5815514 nonzeros.\n",
      "Reduced MIP has 8312 binaries, 0 generals, 0 SOSs, and 0 indicators.\n",
      "Presolve time = 3.01 sec. (2098.19 ticks)\n",
      "Probing fixed 0 vars, tightened 565 bounds.\n",
      "Probing time = 37.69 sec. (3793.72 ticks)\n",
      "Clique table members: 1157722.\n",
      "MIP emphasis: balance optimality and feasibility.\n",
      "MIP search method: dynamic search.\n",
      "Parallel mode: deterministic, using up to 12 threads.\n",
      "Root relaxation solution time = 65.52 sec. (25330.17 ticks)\n",
      "\n",
      "        Nodes                                         Cuts/\n",
      "   Node  Left     Objective  IInf  Best Integer    Best Bound    ItCnt     Gap\n",
      "\n",
      "      0     0   3.74147e+07  3066                 3.74147e+07    20435         \n",
      "      0     0   3.71555e+07  3190                  Cuts: 9998    28783         \n",
      "*     0+    0                       3.21531e+07   3.71555e+07            15.56%\n",
      "      0     0   3.67061e+07  3553   3.21531e+07    Cuts: 9951    48185   14.16%\n",
      "      0     0   3.63634e+07  3594   3.21531e+07    Cuts: 5097    86569   13.09%\n",
      "      0     0   3.60177e+07  3912   3.21531e+07    Cuts: 4531   123404   12.02%\n",
      "      0     0   3.55680e+07  4344   3.21531e+07    Cuts: 5449   182845   10.62%\n",
      "      0     0   3.51943e+07  4557   3.21531e+07    Cuts: 5188   241842    9.46%\n",
      "\n",
      "Clique cuts applied:  1331\n",
      "Implied bound cuts applied:  1982\n",
      "Flow cuts applied:  18\n",
      "Zero-half cuts applied:  438\n",
      "Gomory fractional cuts applied:  3\n",
      "\n",
      "Root node processing (before b&c):\n",
      "  Real time             = 2476.86 sec. (1252463.54 ticks)\n",
      "Parallel b&c, 12 threads:\n",
      "  Real time             =    0.00 sec. (0.00 ticks)\n",
      "  Sync time (average)   =    0.00 sec.\n",
      "  Wait time (average)   =    0.00 sec.\n",
      "                          ------------\n",
      "Total (root+branch&cut) = 2476.86 sec. (1252463.54 ticks)\n",
      "solution for a cost of 32153104.47067618\n",
      "total running time:  1.7333333333333334\n",
      "The maximum profit is  3215.310447067618\n",
      "model complete!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #data_url = 'C:/Users/20937337/Desktop/testData/data/realData/'\n",
    "    #data_url = 'C:/Users/jxhua/OneDrive - Curtin University of Technology Australia/Desktop/toUpload/'\n",
    "    data_url = 'D:/toUpload/'\n",
    "    #bin_num = [30, 30, 30]\n",
    "    bin_num = [22, 30, 35]\n",
    "    grade_margin = 0.2\n",
    "\n",
    "    model, start_t0 = build(bin_num, data_url, grade_margin)\n",
    "\n",
    "    # Solve the model and print solution\n",
    "    solving_t5 = datetime.now()\n",
    "    solution, objective_value = solve(model)\n",
    "    print(\"total running time: \", (solving_t5 - start_t0).seconds/60)\n",
    "    print(\"The maximum profit is \", objective_value/10000)\n",
    "    #print(\"The maximum profit is \", [i/10000 for i in obj_value])\n",
    "    \n",
    "    '''model.excavated = get_solution(model.excavated, solution, 'excavated', trans=True)\n",
    "    model.filled = get_solution(model.filled, solution, 'filled', trans=True)\n",
    "    model.t_m_b_p = get_solution(model.t_m_b_p, solution, 'tons_t_m_b_p')\n",
    "    model.t_m_b_s = get_solution(model.t_m_b_s, solution, 'tons_t_m_b_s')\n",
    "    model.t_m_b_d_l = get_solution(model.t_m_b_d_l, solution, 'tons_t_m_b_d_l')\n",
    "    model.t_s_p = get_solution(model.t_s_p, solution, 'tons_t_s_p')\n",
    "    model.t_s_d_l = get_solution(model.t_s_d_l, solution, 'tons_t_s_d_l')\n",
    "    \n",
    "    model.t_m_b_p.to_csv(data_url+'t_m_b_p.csv', index=False)\n",
    "    model.t_m_b_s.to_csv(data_url+'t_m_b_s.csv', index=False)\n",
    "    model.t_m_b_d_l.to_csv(data_url+'t_m_b_d_l.csv', index=False)\n",
    "    model.t_s_p.to_csv(data_url+'t_s_p.csv', index=False)\n",
    "    model.t_s_d_l.to_csv(data_url+'t_s_d_l.csv', index=False)\n",
    "    model.t_m_b_d_l.loc[model.t_m_b_d_l['var_value']>0].to_csv(data_url+'t_m_b_d_l_var1.csv', index=False)'''\n",
    "    print(\"model complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e770bcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1448.0, 1964.0, 131182.0, 200.0, 154.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.t_m_b_p)/4, len(model.t_m_b_s)/4, len(model.t_m_b_d_l)/4, len(model.t_s_d_l)/4, len(model.t_s_p)/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06bd496b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136010.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.t_m_b_p)/4+len(model.t_m_b_s)/4+len(model.t_m_b_d_l)/4+len(model.t_s_d_l)+len(model.t_s_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f820fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134948.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "539792/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b4d2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.df_blocks.head()\n",
    "\n",
    "len(model.df_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c04b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.df_blocks.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4369f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = model.df_blocks.copy()\n",
    "dd['coef'] = np.where(dd['wting']==1, 1.2, np.where(dd['wting']==2, 1.25, 1.3))\n",
    "dd['volume'] = dd['quantity'] / dd['density'] * dd['coef']\n",
    "dd1 = dd.loc[(dd.if_active==1) & (dd.grade<=0.26)]\n",
    "paf_vol = dd1['volume'].sum()u/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935026a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "paf_vol/25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73ab10a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.excavated = get_solution(model.excavated, solution, 'excavated', trans=True)\n",
    "model.filled = get_solution(model.filled, solution, 'filled', trans=True)\n",
    "model.t_m_b_p = get_solution(model.t_m_b_p, solution, 'tons_t_m_b_p')\n",
    "model.t_m_b_s = get_solution(model.t_m_b_s, solution, 'tons_t_m_b_s')\n",
    "model.t_m_b_d_l = get_solution(model.t_m_b_d_l, solution, 'tons_t_m_b_d_l')\n",
    "model.t_s_p = get_solution(model.t_s_p, solution, 'tons_t_s_p')\n",
    "model.t_s_d_l = get_solution(model.t_s_d_l, solution, 'tons_t_s_d_l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a68c530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1563699.9999999998"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.t_m_b_d_l.var_value.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6cddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = 'D:/toUpload/'\n",
    "bin_num = [22, 30, 35]\n",
    "grade_margin = 0.2\n",
    "\n",
    "model = Model(\"mine_prod\")\n",
    "load_data(model, data_url, bin_num, grade_margin)\n",
    "setup_data(model)\n",
    "setup_variables(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c10733",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(model.t_m_b_p)+len(model.t_m_b_s)+len(model.t_m_b_d_l)+len(model.t_s_p)+len(model.t_s_d_l)+len(model.excavated)+len(model.filled)\n",
    "len(model.t_m_b_p), len(model.t_m_b_s), len(model.t_m_b_d_l), len(model.t_s_p), len(model.t_s_d_l), len(model.excavated), len(model.filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e69471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
